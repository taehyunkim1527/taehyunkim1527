<!DOCTYPE html>
<html lang="en">
  <head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<title>Taehyun Kim</title>
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Taehyun Kim | Hello.</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Taehyun Kim" />
<meta name="author" content="Taehyun Kim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hello." />
<meta property="og:description" content="Hello." />
<link rel="canonical" href="http://mint1.kaist.ac.kr:4000/" />
<meta property="og:url" content="http://mint1.kaist.ac.kr:4000/" />
<meta property="og:site_name" content="Taehyun Kim" />
<script type="application/ld+json">
{"@type":"WebSite","url":"http://mint1.kaist.ac.kr:4000/","headline":"Taehyun Kim","name":"Taehyun Kim","author":{"@type":"Person","name":"Taehyun Kim"},"description":"Hello.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/"><img src="/assets/portfolio.png" alt="Taehyun Kim"></a>
        <h2 id="title">
          <a href="/">Taehyun Kim</a>
        </h2>
        <p class="tagline">Graduate Student</p>
        <ul class="social"><a href="https://github.com/taehyunkim1527">
              <li>
                <i class="icon-github-circled"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/taehyun-kim-9a9911126/">
              <li>
                <i class="icon-linkedin-squared"></i>
              </li>
            </a></ul><p>&copy;
          2020</p>
      </section>
      <section class="content">
        

  <ul class="Intro">
    <div>
      <div>
        <h1>Taehyun Kim</h1>
        <h2>Ph.D. Candidate @ School of Electrical Engineering, KAIST</h2>
        <h3>Room 820, N1 ITC-Building</h3>
        <h3>291 Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea</h3>
        <h3>Contact: taehyunkim1527@gmail.com</h3>
      </div>
    </div><!--// .yui-gc -->
    <hr>
    <h3>Profile</h3>
    Graduate student at KAIST, studying high-performance networked 
    systems and deep learning. Working in Networked & Distributed 
    Computing Systems Lab (NDSL) with my advisor, Prof. KyoungSoo Park, since Feb 2017.
    <hr>
    <h3>Research Interest</h3>
    Systems support for deep learning, scalable graph neural network, scalable networked systems
    <hr>
    <h3>Education</h3>
      Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea
      <ul>
        <li> Ph.D. student, Electrical Engineering  -	Advisor: KyoungSoo Park</li>
        <ul>
          Feb 	2019 –
        </ul>
        <li> M.S. student, Electrical Engineering  -	Advisor: KyoungSoo Park</li>
        <ul>
          Feb 	2017 – 2019
        </ul>
      </ul>
      Yonsei University, Seoul, Republic of Korea
      <ul>
        <li> B.S. student, Electrical Engineering</li>
        <ul>
          Feb 	2011 – 2017
        </ul>
      </ul>
    <hr>
    <h3>Research Projects</h3>
      <b>CoDDL</b>
      <ul>
        CoDDL is a job coordinator on a multi-tenant cluster specialized for distributed deep learning. 
        In order to utilize the resources efficiently even if new jobs starts and existing jobs finish 
        randomly, we should re-distribute the GPUs to jobs whenever necessary. In addition to frequent 
        re-distribution, it is important to take account of lengths and throughput scalability of jobs 
        that varies from model to model. CoDDL designs a scheduling algorithm to improve the job completion 
        time and fairness with above considerations and implements system supports for frequent 
        re-distribution. We have built a stable version of CoDDL. This is an on-going project.
      </ul>
    </br>
      <b>Efficient Data Pipeline for Graph Neural Network</b>
      <ul>
        Recently, large graph data has more than billions of nodes and hundreds of billions of edges. 
        However, GNN frameworks should load the entire graph topology on local memory for high throughput, 
        and therefore we can’t run a model if our memory can’t afford it. Some frameworks by-paths this 
        challenge by sampling the graph or use networked shared memory over multiple servers but they can 
        be less accurate or a waste of resources. I’m conducting a research on designing a graph 
        compression algorithm and efficient pipelining to make GNN training on a large graph more efficient.
        This is an on-going project.
      </ul>
    <hr>
    <h3>Workshops & Posters</h3>
      <ul>
        <li>Changho Hwang, <b>Taehyun Kim</b>, Kyuho Son, Jinwoo Shin, and KyoungSoo Park.</br>
          "Efficient Resource Sharing for Distributed Deep Learning." In the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)-Poster 2018.
        </li>
      </ul>
      <hr>
    <h3>Teaching Experience</h3>
    Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea</br>
    Teaching Assistant, School of Electrical Engineering 
    <ul>
      <li>EE209 Programming Structures for Electrical Engineering - Fall 2018, Spring 2020</li>
      <li>EE817 Advanced Networking and Cloud Systems - Spring 2018</li>
      <li>EE205 Data Structures and Algorithms - Fall 2019</li>
    </ul>
    <hr>
    <h3>Skills</h3>
    C/C++, Python, TensorFlow, PyTorch, Unix/GNU Linux, Spark, LaTeX<hr>


  </ul>
      </section>
    </main></body>
</html>
