<!DOCTYPE html>
<html lang="en">
  <head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<title>Taehyun Kim</title>
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Taehyun Kim | Hello.</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Taehyun Kim" />
<meta name="author" content="Taehyun Kim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hello." />
<meta property="og:description" content="Hello." />
<link rel="canonical" href="http://mint1.kaist.ac.kr:4000/" />
<meta property="og:url" content="http://mint1.kaist.ac.kr:4000/" />
<meta property="og:site_name" content="Taehyun Kim" />
<script type="application/ld+json">
{"@type":"WebSite","url":"http://mint1.kaist.ac.kr:4000/","headline":"Taehyun Kim","name":"Taehyun Kim","author":{"@type":"Person","name":"Taehyun Kim"},"description":"Hello.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
</head>
  <body>
    <main class="container">
      <section class="about">
        <a href="/"><img src="/assets/portfolio.png" alt="Taehyun Kim"></a>
        <h2 id="title">
          <a href="/">Taehyun Kim</a>
        </h2>
        <p class="tagline">Graduate Student</p>
        <ul class="social"><a href="https://github.com/taehyunkim1527">
              <li>
                <i class="icon-github-circled"></i>
              </li>
            </a><a href="https://www.linkedin.com/in/taehyun-kim-9a9911126/">
              <li>
                <i class="icon-linkedin-squared"></i>
              </li>
            </a></ul><p>&copy;
          2020</p>
      </section>
      <section class="content">
        

  <ul class="Intro">
    <div>
      <div>
        <h1>Taehyun Kim</h1>
        <h2>Ph.D. Candidate @ School of Electrical Engineering, KAIST</h2>
        <h3>Room 820, N1 ITC-Building</h3>
        <h3>291 Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea</h3>
      </div>
    </div><!--// .yui-gc -->
    <hr>
    <h3>Profile</h3>
    Graduate student at KAIST, studying high-performance networked 
    systems and deep learning. Working in Networked & Distributed 
    Computing Systems Lab (NDSL) with my advisor, Prof. KyoungSoo Park, since Feb 2017.
    <hr>
    <h3>Research Interest</h3>
    Systems support for deep learning, scalable graph neural network, scalable networked systems
    <hr>
    <h3>Education</h3>
    <li>
      Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea
      Ph.D. student, Electrical Engineering 				Feb 	2020 –
      -	Advisor: KyoungSoo Park
      M.S. student, Electrical Engineering 				Feb 	2017 – 2019
      -	Advisor: KyoungSoo Park
    </li>
    <li>
      Yonsei University, Seoul, Republic of Korea
      B.S., Electrical Engineering					Feb 	2011 – 2017
    </li>
    <hr>
    <h3>Research Projects</h3>
    CoDDL: Dynamic Resource Sharing for Distributed Deep Learning on the Multi-tenant Cluster May 2017–
    CoDDL is a job coordinator on a multi-tenant cluster specialized for distributed deep learning. 
    It supports auto-parallelization to run each deep learning job on multiple GPUs and 
    fast re-adjustment of the number of GPUs for a job to let it yield or add GPUs in runtime. 
    Therefore, even if some jobs come in and out frequently, schedulers can maintain high utilization 
    of cluster resources by calculating the best allocation consistently. 
    We have built a stable version of CoDDL. This is an on-going project.

    Efficient Data Pipeline for Graph Neural Network Dec 2019–
    Recent large graph data structure has more than billions of nodes and hundreds of billions of edges.
    Therefore, GNN frameworks can’t load the entire graph topology on local memory and it harms 
    the training throughput. Some frameworks by-paths this challenge by sampling the graph or 
    use networked shared memory over multiple servers but they can be less accurate or 
    a waste of resources. I’m conducting research on designing a graph compression algorithm and 
    efficient pipelining to make GNN training on a large graph more efficient. 
    This is an on-going project.
    <hr>
    <h3>Workshops & Posters</h3>
    Changho Hwang, Taehyun Kim, Kyuho Son, Jinwoo Shin, and KyoungSoo Park.
Efficient Resource Sharing for Distributed Deep Learning. In the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)-Poster 2018.
    <hr>
    <h3>Teaching Experience</h3>
    Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea
    Teaching Assistant, School of Electrical Engineering 
    	EE209 Programming Structures for Electrical Engineering - Fall 2018, Spring 2020
    	EE817 Advanced Networking and Cloud Systems - Spring 2018
    	EE205 Data Structures and Algorithms - Fall 2019

    <hr>
    <h3>Skills</h3>
    C/C++, Python, TensorFlow, PyTorch, Unix/GNU Linux, Spark, LaTeX<hr>


  </ul>
      </section>
    </main></body>
</html>
